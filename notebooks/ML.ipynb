{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33abb41e-bd02-4ce0-9f78-2d23dbe8ffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.9.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44cdaaf-ba0d-4f08-83cf-4375e5197a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0be87af-934a-4365-a4d3-740e60ea0d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace, length\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from langdetect import detect, LangDetectException\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# 1. Créer une session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Mastodon Sentiment Analysis\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/postgresql-42.3.9.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Configurer les paramètres de connexion à la base PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/DB_Mastodon\"\n",
    "properties = {\n",
    "    \"user\": \"fadi\",\n",
    "    \"password\": \"fadi\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# 3. Lire les données depuis PostgreSQL\n",
    "df_gold = spark.read.jdbc(url=jdbc_url, table=\"Mostodon_GOLD_bis\", properties=properties)\n",
    "\n",
    "# 4. Détection de langue et nettoyage\n",
    "\n",
    "# Fonction pour détecter la langue avec langdetect\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return 'Other'\n",
    "\n",
    "# UDF pour la détection de langue\n",
    "detect_language_udf = udf(detect_language, StringType())\n",
    "\n",
    "# Ajouter une colonne \"Lang\" pour la langue détectée\n",
    "df_gold = df_gold.withColumn(\"Lang\", detect_language_udf(col(\"content\")))\n",
    "\n",
    "# Filtrer les données pour ne garder que les textes en anglais\n",
    "df_gold = df_gold.filter(df_gold.Lang == 'en')\n",
    "\n",
    "# Nettoyage de texte (suppression des URLs, mise en minuscules et suppression des caractères spéciaux)\n",
    "df_gold = df_gold.withColumn('content', lower(col('content')))\n",
    "df_gold = df_gold.withColumn('content', regexp_replace('content', 'https?://\\S+|www\\.\\S+', ''))\n",
    "df_gold = df_gold.withColumn('content', regexp_replace('content', '[^a-zA-Z\\s]', ''))\n",
    "df_gold = df_gold.withColumn('content', regexp_replace('content', '\\s+', ' '))\n",
    "\n",
    "# Supprimer les textes trop courts (optionnel)\n",
    "df_gold = df_gold.filter(length(col('content')) > 5)\n",
    "\n",
    "# 5. Prétraitement du texte avec PySpark ML\n",
    "\n",
    "# Tokenizer: Diviser le texte en mots\n",
    "tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "tokenized_data = tokenizer.transform(df_gold)\n",
    "\n",
    "# Supprimer les mots vides (stopwords)\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "data_no_stopwords = remover.transform(tokenized_data)\n",
    "\n",
    "# Convertir les mots en vecteurs de caractéristiques avec CountVectorizer\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
    "vectorized_data = vectorizer.fit(data_no_stopwords).transform(data_no_stopwords)\n",
    "\n",
    "# Utiliser TF-IDF pour pondérer les mots\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "tfidf_data = idf.fit(vectorized_data).transform(vectorized_data)\n",
    "\n",
    "# 6. Analyse de sentiment avec VADER\n",
    "\n",
    "# Instancier VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Fonction pour obtenir le sentiment avec VADER\n",
    "def get_vader_sentiment(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    if scores['compound'] > 0.05:\n",
    "        return 'Positive'\n",
    "    elif scores['compound'] < -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# UDF pour l'analyse de sentiment avec VADER\n",
    "get_vader_sentiment_udf = udf(get_vader_sentiment, StringType())\n",
    "\n",
    "# Ajouter une colonne \"Sentiment\" avec le résultat de VADER\n",
    "df_gold = df_gold.withColumn(\"Sentiment\", get_vader_sentiment_udf(col(\"content\")))\n",
    "\n",
    "# 7. Enregistrer les résultats dans PostgreSQL\n",
    "\n",
    "# Sélectionner les colonnes id, username, content, et sentiment\n",
    "final_df = df_gold.select('id', 'username', 'content', 'Sentiment').toPandas()\n",
    "\n",
    "# Enregistrer dans PostgreSQL\n",
    "engine = create_engine('postgresql://fadi:fadi@postgres:5432/DB_Mastodon')\n",
    "final_df.to_sql('sentiment_analysis_results', engine, if_exists='replace', index=False)\n",
    "\n",
    "# Fermer la session Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c62e95-bee1-42ff-a02c-565664f38948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
